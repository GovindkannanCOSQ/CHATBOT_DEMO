{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GovindkannanCOSQ/CHATBOT_DEMO/blob/main/CHAT_BOT_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rNa-qpXVbfI"
      },
      "outputs": [],
      "source": [
        "!pip install cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anwEKIzEVlGi"
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykBQnRjoV4bV"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiPSS2pRWIrC"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R-JPrVXWPuV"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekwupgEESAxh"
      },
      "outputs": [],
      "source": [
        "!pip install colorama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr3a91iHnvI1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "# Set OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Load documents\n",
        "documents = SimpleDirectoryReader(\"drive/MyDrive/mydata\").load_data()\n",
        "\n",
        "# Create vector store index\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Persist the index\n",
        "index.storage_context.persist()\n",
        "\n",
        "system_prompt = \"You are a Travel Enquiry assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
        "\n",
        "# Create OpenAI model\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=256)\n",
        "\n",
        "# Set up service context\n",
        "from llama_index import ServiceContext, set_global_service_context\n",
        "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=800, chunk_overlap=20)\n",
        "set_global_service_context(service_context)\n",
        "\n",
        "# Recreate vector store index with service context\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "# Create a query engine with streaming\n",
        "query_engine = index.as_query_engine(streaming=True)\n",
        "\n",
        "# User interaction loop\n",
        "print(\"Welcome to the Travel Chatbot!\")\n",
        "print(\"I'm Himico,Ask me anything!\")\n",
        "\n",
        "conversation_history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"HAPPY JOURNEY!!\")\n",
        "        break\n",
        "\n",
        "    # Add the user input to the conversation history\n",
        "    conversation_history.append(f\"You: {user_input}\")\n",
        "\n",
        "    # Concatenate the conversation history into a single string\n",
        "    conversation_str = \"\\n\".join(conversation_history)\n",
        "\n",
        "    # Perform query and get response with conversation history\n",
        "    response = query_engine.query(conversation_str)\n",
        "\n",
        "    # Check if the response is empty\n",
        "    if not response:\n",
        "        # Use the general OpenAI model to respond to the user's question\n",
        "        general_response = llm.ask(user_input)\n",
        "        response = f\"Himico: {general_response}\"\n",
        "\n",
        "    # Display response\n",
        "    print(\"Himico:\",response)\n",
        "\n",
        "    # Add the chatbot response to the conversation history\n",
        "    conversation_history.append(f\"Himico: {response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "VVspltw09kxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade typing-extensions\n"
      ],
      "metadata": {
        "id": "Z6nGtXG_91zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U typing-extensions\n"
      ],
      "metadata": {
        "id": "kg62mRFdGEYI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show typing-extensions\n"
      ],
      "metadata": {
        "id": "CGvQD3e1NP-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade typing-extensions\n"
      ],
      "metadata": {
        "id": "MghZFhLnNWU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from llama_index.llms import OpenAI\n",
        "from typing_extensions import Doc\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, set_global_service_context\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/drive/MyDrive/mydata\").load_data()\n",
        "\n",
        "# Create vector store index\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Persist the index\n",
        "index.storage_context.persist()\n",
        "\n",
        "system_prompt = \"You are a Travel Enquiry assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
        "\n",
        "# Create OpenAI model\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=256)\n",
        "\n",
        "# Set up service context\n",
        "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=800, chunk_overlap=20)\n",
        "set_global_service_context(service_context)\n",
        "\n",
        "# Recreate vector store index with service context\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "# Create a query engine with streaming\n",
        "query_engine = index.as_query_engine(streaming=True)\n",
        "\n",
        "\n",
        "print(\"Welcome to the Travel Chatbot!\")\n",
        "print(\"I'm Himico, Ask me anything!\")\n",
        "\n",
        "conversation_history = []\n",
        "\n",
        "\n",
        "def chatbot_interaction(user_input):\n",
        "    global conversation_history\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        return \"HAPPY JOURNEY!!\"\n",
        "\n",
        "\n",
        "    conversation_history.append(f\"You: {user_input}\")\n",
        "\n",
        "\n",
        "    conversation_str = \"\\n\".join(conversation_history)\n",
        "\n",
        "\n",
        "    response = query_engine.query(conversation_str)\n",
        "\n",
        "    # Check if the response is empty\n",
        "    if not response:\n",
        "        # Use the general OpenAI model to respond to the user's question\n",
        "        general_response = llm.ask(user_input)\n",
        "        response = f\"Himico: {general_response}\"\n",
        "\n",
        "    # Add the chatbot response to the conversation history\n",
        "    conversation_history.append(f\"Himico: {response}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Create Gradio interface with a chat-style input and output\n",
        "iface = gr.Interface(fn=chatbot_interaction, inputs=gr.Textbox(label=\"You: \"), outputs=gr.Textbox(label=\"Himico: \"))\n",
        "iface.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "WFF2aiKeIM6k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "22d8f1dd-1f4f-4bb3-eca8-8cbd0ecdaa7f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Travel Chatbot!\n",
            "I'm Himico, Ask me anything!\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ed771031c6a3eae126.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ed771031c6a3eae126.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hB8iHFqR67EynDZLbr24PqSDP5h_GU7O",
      "authorship_tag": "ABX9TyOIau9cPbhBDhSKxehHxmEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}